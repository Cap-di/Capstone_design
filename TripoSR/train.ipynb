{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ba704ce-7539-4d32-bf19-55fd09958526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import rembg\n",
    "import torch\n",
    "import lpips\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tsr.system import TSR\n",
    "from tsr.utils import remove_background, resize_foreground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ffdb127-9958-4297-b22b-42b28c161119",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"test/\"\n",
    "\n",
    "images=[]\n",
    "rembg_session = rembg.new_session()\n",
    "\n",
    "image = remove_background(Image.open(\"./examples/chair.png\"), rembg_session)\n",
    "image = resize_foreground(image, 0.85)\n",
    "image = np.array(image).astype(np.float32) / 255.0\n",
    "image = image[:, :, :3] * image[:, :, 3:4] + (1 - image[:, :, 3:4]) * 0.5\n",
    "image = Image.fromarray((image * 255.0).astype(np.uint8))\n",
    "image.save(os.path.join(output_dir, f\"input.png\"))\n",
    "images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dbe174e-68b7-40fb-a847-993f11237313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = TSR.from_pretrained(\n",
    "    \"./train\",\n",
    "    config_name=\"config.yaml\",\n",
    "    weight_name=\"model.ckpt\",\n",
    ")\n",
    "\n",
    "# Set parameters\n",
    "chunkSize = 8192 # Chunk size\n",
    "nViews = 30 # Number of views\n",
    "mcResolution = 256 # Marching cubes\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device==\"cuda:0\": torch.cuda.empty_cache()\n",
    "model.to(device)\n",
    "model.renderer.set_chunk_size(chunkSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf3a81a-34ed-4c8d-94f1-f4658478d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image in enumerate(images):\n",
    "    with torch.no_grad():\n",
    "        scene_codes = model([image], device=device)\n",
    "        \n",
    "    render_images = model.render(scene_codes, n_views=nViews, return_type=\"pil\")\n",
    "    for ri, render_image in enumerate(render_images[0]):\n",
    "        render_image.save(os.path.join(output_dir, f\"render_{ri:03d}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afab6e3e-84b7-43e3-b1db-61d133ba03d6",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79028906-c9c9-4848-bb53-bc7000c45630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/anaconda3/envs/tsr/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/tony/anaconda3/envs/tsr/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/tony/anaconda3/envs/tsr/lib/python3.8/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    }
   ],
   "source": [
    "# 옵티마이저 및 손실 함수 설정\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)    # weight_decay is L2 regularization\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss_fn_vgg = lpips.LPIPS(net='vgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19b6e307-a1b3-4764-b143-c69bdf6fc104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the loss\n",
    "def compute_loss(self, render_out, render_gt):\n",
    "    # NOTE: the rgb value range of OpenLRM is [0, 1]\n",
    "    render_images = render_out['render_images']\n",
    "    target_images = render_gt['target_images'].to(render_images)\n",
    "    render_images = rearrange(render_images, 'b n ... -> (b n) ...') * 2.0 - 1.0\n",
    "    target_images = rearrange(target_images, 'b n ... -> (b n) ...') * 2.0 - 1.0\n",
    "\n",
    "    loss_mse = F.mse_loss(render_images, target_images)\n",
    "    loss_lpips = 2.0 * loss_fn_vgg(img0, img1)\n",
    "\n",
    "    render_alphas = render_out['render_alphas']\n",
    "    target_alphas = render_gt['target_alphas']\n",
    "    loss_mask = F.mse_loss(render_alphas, target_alphas)\n",
    "\n",
    "    loss = loss_mse + loss_lpips + loss_mask\n",
    "\n",
    "    prefix = 'train'\n",
    "    loss_dict = {}\n",
    "    loss_dict.update({f'{prefix}/loss_mse': loss_mse})\n",
    "    loss_dict.update({f'{prefix}/loss_lpips': loss_lpips})\n",
    "    loss_dict.update({f'{prefix}/loss_mask': loss_mask})\n",
    "    loss_dict.update({f'{prefix}/loss': loss})\n",
    "\n",
    "    return loss, loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc300dfd-be86-436c-b445-0a20c27694b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(100):  # number of epochs\n",
    "    for image in images:\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            scene_codes = model([image], device=device)\n",
    "\n",
    "        render_images = model.render(scene_codes, n_views=nViews, return_type=\"pil\")\n",
    "        meshes = model.extract_mesh(scene_codes, resolution=mcResolution)\n",
    "\n",
    "        compute_loss()\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss_mask.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdbea83-4426-40eb-97b9-f3807e7a53b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
